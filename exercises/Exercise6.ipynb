{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6\n",
    "## Random numbers\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "How can I generate random numbers, and what are they useful for?\n",
    "\n",
    "### Objectives\n",
    "<div class=obj>\n",
    "<ol>\n",
    "    <li>Draw random numbers.</li>\n",
    "    <li>Use random numbers to generate probability distributions.</li>\n",
    "    <li>Learn the basics of numerical error propagation</li>\n",
    "</ol>\n",
    "    \n",
    "<ul>\n",
    "Revise:\n",
    "    <li>Plotting (histograms);</li>\n",
    "    <li>Fitting regressions;</li>\n",
    "    <li>Defining functions;</li>\n",
    "    <li>If else statements;</li>\n",
    "    <li>Data readin.</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "### Independent coding\n",
    "Error propagation through magmatic crystallisation temperature estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Generating randomness\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's jump right in and generate a random number, using the `random` library (__[see here for details](https://docs.python.org/3/library/random.html#module-random)__)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rand\n",
    "\n",
    "#randint(a,b) will generate \n",
    "rand.randint(0,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep re-running the above cell, does the number change?\n",
    "\n",
    "You might wonder how a computer is randomly generating a number.  In fact, computers are generally totally dertministic in their random number generation.  In this case, we are creating a list of numbers between 0 and 100, python is generating a pseudo-random number between 0.0 and 1.0 and using that to choose an element from our list.  The underlying random number is being chosen using the system time to 'seed' the draw between 0.0 and 1.0 - that is why if you keep running the cell you obtain a different number.  \n",
    "\n",
    "Let's set the seed and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's generate two bits of code to determine how random numbers are drawn\n",
    "\n",
    "#1. repeat drawing but allowing the random seed to be drawn from the system time\n",
    "print('test 1')\n",
    "#explicitly set seed to empty, which defaults to system time\n",
    "rand.seed()\n",
    "for i in range(0,10):\n",
    "    print(rand.randint(0,100))\n",
    "\n",
    "#2. set random seed - why did I choose this number?\n",
    "rand.seed(1209)\n",
    "\n",
    "print('\\ntest 2')\n",
    "for i in range(0,10):\n",
    "    print(rand.randint(0,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above cell multiple times and you will see that the first set of numbers changes with every run.  However, the second set of numbers stays fixed.  In other words, if you know the seed (and random generator algorithm) that is being used to generate a set of random numbers, then the outcome is entirely deterministic.  This is the reason that Python's simple random package _should not_ be used for cryptographic purposes and that it calls its random numbers 'pseudo-random'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at plotting some of the results of our random number generator so we can see the __probability distribution__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#to visiualise a distribution we are going to have to draw from the distribution repeatedly.\n",
    "# Let's initially set the number of draws to 40\n",
    "Ndraws = 40\n",
    "\n",
    "#initialise numpy array of zeros\n",
    "x = np.zeros(Ndraws)\n",
    "for i in range(0,Ndraws):\n",
    "    x[i] = rand.randint(0,100)\n",
    "    \n",
    "#now plot the results as a histogram, setting the bin width to 1 in the left hand histogram so it is clear which numbers have been randomly selected\n",
    "# leaving the bin width as automatic (=10 bins) to get a better sense of the shape of the distribution in the right hand plot\n",
    "fg, ax = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "ax[0].hist(x, bins=100);\n",
    "ax[1].hist(x)\n",
    "\n",
    "#label axes\n",
    "for a in ax:\n",
    "    a.set_xlabel('Number drawn')\n",
    "\n",
    "#set width of sub plots \n",
    "fg.set_figwidth(10)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the distribution looks quite lumpy.  If you re-run it, you will frequently find certain numbers that have been picked twice and many numbers that have been picked 0 times. \n",
    "\n",
    "To obtain a smoother distribution requires more sampling, so let's re-run with more draws from the distribution.  We will also switch the random number generator we are using to something a little more convenient to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's use a numpy random number generator this time, so we can skip the for loop\n",
    "Ndraws = 1000\n",
    "\n",
    "x = np.random.randint(0,100,Ndraws)\n",
    "\n",
    "fg, ax = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "ax[0].hist(x, bins=100);\n",
    "ax[1].hist(x)\n",
    "\n",
    "#let's also draw a horizontal line on, marking our prediction of what the underlying probability distribution looks like.\n",
    "# why have I set the y-height as Ndraws/100 for the left hand plot?\n",
    "ax[0].hlines(Ndraws/100,0,100, color='red')\n",
    "# why have I set the y-height as Ndraws/100*10 for the right hand plot?\n",
    "ax[1].hlines(Ndraws/100*10,0,100, color='red')\n",
    "\n",
    "#label axes\n",
    "for a in ax:\n",
    "    a.set_xlabel('Number drawn')\n",
    "\n",
    "#set width of sub plots \n",
    "fg.set_figwidth(10)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should look smoother than the plots above.  In particular, the right hand plot that has wider bins and thereby averages the draws over several numbers, should be beginning to look more like a __[uniform distribution](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous))__.\n",
    "\n",
    "Try changing the number of draws, how high do you have to set the number before you have a distribution that looks the same as our prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Types of distributions\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we saw how to generate random integers, and how we can plot these to visualise the distribution we are drawing from.  \n",
    "\n",
    "The uniform distribution we looked at is an important distribution, expressing equal preference (or weight) for numbers within its range, and zero weight for all numbers outside.  This is a useful distribution when you have some parameter that can vary between specific bounds, but not outside.  For example, the age of stars in our galaxy are constrained to be between 0 and the age of the universe.  Not knowing anything else, you might therefore consider their ages to be described by a uniform distribution.  \n",
    "\n",
    "A second distribution that comes up time and time again is the __[normal distribution](https://en.wikipedia.org/wiki/Normal_distribution)__.  This is particular useful in data science because the uncertainty on data are often assumed (or known) to be normally distributed.  So, simulating the intrinsic noise on data can be achieved drawin from the normal distribution. \n",
    "\n",
    "Let's first look at the normal distribution, centred on 0 and with unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the numpy normal distribution takes arguments location, scale, size\n",
    "# location = mean\n",
    "# scale = 1 sigma\n",
    "# size = number of points to be generated\n",
    "mean = 0\n",
    "sig = 1\n",
    "\n",
    "#setup subplots\n",
    "fg, ax = plt.subplots(nrows=1, ncols=2)\n",
    "fg.set_figwidth(10)\n",
    "\n",
    "#plot two sets of random draws, the right hand plot drawing 10 times more from the underlying distributino\n",
    "# note that plotting a histogram returns a series of objects, look at the matplotlib.pyplot.hist page to \n",
    "# understand what these are.  For our purposes the one we want to access later is the bins (stored in b1 and b2), so we can calculate\n",
    "# the bin width and use that to plot our underlying distribution at the correct y-axis height.\n",
    "n, b1, p = ax[0].hist(np.random.normal(mean,sig,100))\n",
    "n, b2, p = ax[1].hist(np.random.normal(mean,sig,1000))\n",
    "\n",
    "#set common x-range to the plots at +/- 5 sigma\n",
    "for a in ax:\n",
    "    a.set_xlim(-5,5)\n",
    "\n",
    "#plot on theoretical distribution, recalling the definition of a normal distribution\n",
    "# np.linspace creates a linearly spaced series of numbers between limits in a numpy array.\n",
    "x = np.linspace(-5,5,1000)\n",
    "# create array of zeros to fill in\n",
    "y = np.zeros(1000)\n",
    "# enumerate just counts where we are through the array of x values\n",
    "for i, xi in enumerate(x):\n",
    "    y[i] = 1/(sig*(2*np.pi)**(0.5)) * np.exp(-0.5 * ((xi-mean)/sig)**2)\n",
    "\n",
    "#now plot underlying normal distribution\n",
    "ax[0].plot(x, y*100*(b1[1]-b1[0]), color='red')\n",
    "ax[1].plot(x, y*1000*(b2[1]-b2[0]), color='red')\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that this is now a __continuous__ distribution.  Whilst previously we were drawing uniformly from integers between 0 and 100, our draws now lie randomly along the real number line concentrated around the mean of the distribution.  \n",
    "\n",
    "There are many pre-computed __[distributions available](https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.random.html)__ for use through NumPy, which are useful for various statistical tests you may come across.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Error propagation\n",
    "***\n",
    "\n",
    "Random number generation is enormously useful for error propagation.  Let's now look at an example of this using a real dataset and see how drawing from the normal distribution can be useful in quantifying our uncertainty in properties of the data.\n",
    "\n",
    "In __[Exercise 5](Exercise5.ipynb)__ we look at fitting isochrons.  Let's revist this, but now using our knowledge of random sampling to estimate the uncertainty on the isochron age.  \n",
    "\n",
    "First, we will replot the data, and compare it to a plot where the data are randomly purturbed according to their uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------same as in Exercise 5---------------------------#\n",
    "import scipy.stats as sts\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('./data/Sierra_nevada_granodiorite.txt',sep=',',header=0)\n",
    "\n",
    "#calculate regression through data\n",
    "res = sts.linregress(df.rbsr8786, df.srsr8786)\n",
    "\n",
    "def lfunc(x, m, c):\n",
    "    y = m*x + c\n",
    "    return y\n",
    "\n",
    "fg, ax = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "#make plot with error bars\n",
    "ax[0].errorbar(df.rbsr8786, df.srsr8786, yerr=df.srsr_1sig, xerr=(df.rbsr_1sigpc*df.rbsr8786/100), \n",
    "            color='white', marker='o', markeredgecolor='black', linestyle='None',\n",
    "           ecolor='black')\n",
    "\n",
    "#-----------------------------------------------------#\n",
    "#now, let's replot the data randomly purturbing their location according to their error\n",
    "\n",
    "# first, we will plot the original data in the background so we can see how points have shifted\n",
    "ax[1].scatter(df.rbsr8786, df.srsr8786, color='lightgrey', marker='o', linestyle='None')\n",
    "\n",
    "#now let's generate our randomly shifted dataset\n",
    "# first the x data, note we can pass the numpy random function arrays of mean values and standard deviations, so we can resample the data in one line\n",
    "x = np.random.normal(df.rbsr8786, df.rbsr_1sigpc*df.rbsr8786/100)\n",
    "# then the y data the same way\n",
    "y = np.random.normal(df.srsr8786, df.srsr_1sig)\n",
    "\n",
    "#Now plot the new data\n",
    "ax[1].scatter(x,y, marker='o', color='red')\n",
    "\n",
    "#tidying up appearance\n",
    "for a in ax:\n",
    "    a.set_ylim([0.706,0.708]);\n",
    "    a.set_xlabel(r'${^{87}Rb/^{86}Sr}$')\n",
    "    a.set_ylabel(r'${^{87}Sr/^{86}Sr}$');\n",
    "    a.grid(ls=':');\n",
    "\n",
    "\n",
    "fg.set_figwidth(10)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our randomly resampled data lie displced from the original data.  What's more, if you rerun the cell above you will see the data shift around each time, as a new draw from the distribution is made.\n",
    "\n",
    "This might seem useful for visualisation purposes but otherwise a little useless.  However, the power of random sampling is in calculating how this uncertainty propagates through to a calculation we perform on the data.  For the isochron example what we care about is the age.  So let's now use random resampling to calculate the error on the ages that we obtain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg, ax = plt.subplots(nrows=1, ncols=1)\n",
    "\n",
    "#make plot with error bars\n",
    "ax.errorbar(df.rbsr8786, df.srsr8786, yerr=df.srsr_1sig, xerr=(df.rbsr_1sigpc*df.rbsr8786/100), \n",
    "            color='white', marker='o', markeredgecolor='black', linestyle='None',\n",
    "           ecolor='black', zorder=2)\n",
    "xlim = ax.get_xlim()\n",
    "\n",
    "#decay constant for Rb -> Sr decay; needed for calculating ages\n",
    "lmbd = 1.393e-11\n",
    "\n",
    "#Let's repeat the regression calculation 1000 times with a new and randomly purturbed dataset in each case.\n",
    "# create list object to hold results of all the regression calculations\n",
    "res = []\n",
    "# create a numpy array to store the ages we calculate\n",
    "ages = np.zeros(1000)\n",
    "for i in range(0,1000):\n",
    "    #generate new data\n",
    "    x = np.random.normal(df.rbsr8786, df.rbsr_1sigpc*df.rbsr8786/100)\n",
    "    y = np.random.normal(df.srsr8786, df.srsr_1sig)\n",
    "    res.append(sts.linregress(x, y))\n",
    "    \n",
    "    ages[i] = np.log(res[-1].slope + 1)/lmbd\n",
    "\n",
    "    #plot regression line using gradient and intercept just calculated\n",
    "    # recall that [-1] accesses the last element in an list\n",
    "    x = np.linspace(xlim[0], xlim[1], len(y))\n",
    "    y = lfunc(x, res[-1].slope, res[-1].intercept)\n",
    "    ax.plot(x, y, linestyle='-', color='lightgrey', zorder=1)\n",
    "    \n",
    "#we will overlay the original regression line, as calculated using the mothod of Exercise 5\n",
    "res_orig = sts.linregress(df.rbsr8786, df.srsr8786)\n",
    "x = np.linspace(xlim[0], xlim[1], 100)\n",
    "y = lfunc(x, res_orig.slope, res_orig.intercept)\n",
    "ax.plot(x,y, linestyle='-', color='black',zorder=1)\n",
    "\n",
    "#now let's write the result onto the plot\n",
    "text = 'age (random sampling): {:.1f} $\\pm$ {:.0f} Myr'.format(np.mean(ages)/1e6, np.std(ages)/1e6)\n",
    "ax.text(0.9,0.1, text, transform=ax.transAxes, horizontalalignment='right',verticalalignment='bottom')\n",
    "# and put the original result on as well\n",
    "text = 'age (raw data): {:.1f} Myr'.format((np.log(res_orig.slope + 1)/lmbd)/1e6)\n",
    "ax.text(0.9,0.05, text, transform=ax.transAxes, horizontalalignment='right',verticalalignment='bottom')\n",
    "\n",
    "ax.set_ylim([0.706,0.708]);\n",
    "ax.set_xlabel(r'${^{87}Rb/^{86}Sr}$')\n",
    "ax.set_ylabel(r'${^{87}Sr/^{86}Sr}$');\n",
    "ax.grid(ls=':');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the average age from the 1000 random realisations is close (perhaps even the same) as the age obtained from the regression through the raw data.  What happens if you increase the number of random realisations?  Does it converge (and stablise) on the same average age as the regression of the raw data?\n",
    "\n",
    "We have also plotted each of the regression lines generated, which gives a sense of the envelope of possible regression lines that could be fit through the data, given their uncertainty.\n",
    "\n",
    "The analysis performed here is often called __Monte Carlo__ sampling, a technique __[born during the Manhattan project](https://en.wikipedia.org/wiki/Monte_Carlo_method#History)__.  Despite the name and the technique's history, it is no more complex than __repeat random sampling__.  Despite the simplicity, it is a very powerful tool in scientific computing.\n",
    "\n",
    "<div class=obj>\n",
    "    <b>Note:</b> We haven't performed a complete error analysis on this data.  Although far better than just reporting the age as 89.4 Myr with no uncertainty, we haven't accounted for the correlated error on x and y that arises from their common denominator.  In this case the correlated uncertainty is unlikely to make a significant difference, because the x error is so much smaller than the y error.  This isn't always the case, for example with U-Pb dating.\n",
    "</div>\n",
    "<p></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Random sampling of objects\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final example we will look at is random sampling of objects.  To illustrate this we will code up a classic gameshow gamble.  The setup is this:\n",
    "\n",
    "> The host of the show sets up three identical boxes.  Two boxes are empty, and in the third is a beautiful piece of G5a, Shap granite, totally priceless.  The host knows where the specimen is and invites you to pick a box.  The host then picks _another_ box, opens it and shows you it is empty.  \n",
    ">\n",
    "> The host then gives you a choice, 'keep your first choice, or select the remaining box?'.\n",
    ">\n",
    "> What should you do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we could reason out the answer to this puzzle.  But we can code we don't have to reason! Let's get the computer to do it.\n",
    "\n",
    "We are going to write code to randomly populate three boxes, randomly pick one box, and then vary between systematically switching the box we have picked vs. sticking with the original box we picked.  In both cases we will calculate our win percentage, and from this know what to do if ever faced with this high stakes gamble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rand\n",
    "\n",
    "#setup boxes\n",
    "box = ['sadness', 'sadness', 'shapp']\n",
    "#setup static index array to box\n",
    "boxid = [0, 1, 2]\n",
    "\n",
    "#let's play this game a lot\n",
    "Ngames = 1000\n",
    "# we will need to count how many times we win\n",
    "win_count = 0\n",
    "for i in range(0,Ngames):\n",
    "    #shuffle the box\n",
    "    box = rand.sample(box,3)\n",
    "    \n",
    "    #pick our box\n",
    "    pick = rand.sample(boxid,1)[0]\n",
    "\n",
    "    #now the host needs to pick their box, and they will always pick and empty box\n",
    "    for i, b in enumerate(box):\n",
    "        if i != pick and b == 'sadness':\n",
    "            show = i\n",
    "\n",
    "    #now, we decide to stick or switch to the remaining box\n",
    "    # let's first see what happens if we stick\n",
    "    if box[pick] == 'shapp':\n",
    "        win_count = win_count + 1\n",
    "\n",
    "print('Sticking, we won', win_count/Ngames*100, ' percent of the time')\n",
    "\n",
    "#now let's look at switching\n",
    "# remembering to reset our win counter\n",
    "win_count = 0\n",
    "for i in range(0,Ngames):\n",
    "    #reset the box, this method modifies the box each step, so it needs resetting\n",
    "    box = ['sadness', 'sadness', 'shapp']\n",
    "    #shuffle the box\n",
    "    box = rand.sample(box,3)\n",
    "    #reset box id's\n",
    "    boxid = [0,1,2]\n",
    "    \n",
    "    #pick our box\n",
    "    pick = rand.sample(boxid,1)[0]\n",
    "\n",
    "    #now the host needs to pick their box, and they will always pick an empty box\n",
    "    for i, b in enumerate(box):\n",
    "        if i != pick and b == 'sadness':\n",
    "            show = i\n",
    "\n",
    "    #now, we decide to stick or switch to the remaining box\n",
    "    # this time we switch\n",
    "    # to code this we are going to delete the elements from the box we have picked and the host has \n",
    "    # showed us, just leaving us with the remaining box.\n",
    "    for i in sorted([pick, show], reverse=True):\n",
    "        del box[i]\n",
    "\n",
    "    if box[0] == 'shapp':\n",
    "        win_count = win_count + 1\n",
    "\n",
    "print('Switching, we won', win_count/Ngames*100, ' percent of the time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you make sense of this result?\n",
    "\n",
    "In anycase, here is some digital G5a as a reward for getting this far.\n",
    "\n",
    "![shapp](img/shap_granite_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent coding\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=obj>\n",
    "    <b>Aim:</b> To use a Monte Carlo methods to propagate error through a calculation.\n",
    "</div>\n",
    "<p></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common problem where random numbers come in useful is when needing to propagate error through a calculation or series of calculations.  Rather than __[formal error propagation](https://en.wikipedia.org/wiki/Propagation_of_uncertainty)__ it is often easier to make the computer do all the work, and recalculate multiple times with the input randomly pertured. \n",
    "\n",
    "Let's look at this now for calculating the crystallisation temperature of magmatic olivines.  This topic has been an ongoing source of controversy, because of it being the first step in calculating mantle potential temperature and the debate over whether mantle plumes are hotter than ambient mantle (see __[Matthews et al. (2016)](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1002/2016GC006497)__ for a recent perspective).  \n",
    "\n",
    "A popular method for calculating magamatic temperatures uses the temperature dependent partitioning of Al between spinel and olivine to estimate the temperature crystals grow at.  The olivine-spinel Al partitioning has been experimentally investigated by __[Wan et al. 2008](https://doi.org/10.2138/am.2008.2758)__ and __[Coogan et al. 2014](http://dx.doi.org/10.1016/j.chemgeo.2014.01.004)__ leading to\n",
    "\n",
    "\\begin{align}\n",
    "T(\\mathrm{K}) = \\frac{10^4}{0.575(0.162) + 0.884(0.043)Y_\\mathrm{Cr} - 0.897(0.025)\\ln(\\mathrm{k_d})},\n",
    "\\end{align}\n",
    "\n",
    "where the numbers in parentheses are the 1 sigma on the parameter estimate.  $Y_\\mathrm{Cr}$ is the Cr number of the spinel, and is given by \n",
    "\n",
    "\\begin{align}\n",
    "Y_\\mathrm{Cr}= \\frac{X_\\mathrm{Cr}}{(X_\\mathrm{Cr}+X_\\mathrm{Al})},\n",
    "\\end{align}\n",
    "\n",
    "where $X_i$ is the _atomic_ abundance of element $i$.  Atomic abundances can be calculated from weight fractions by dividing each oxide component present in the mineral's composition by their mean molecular mass and renomalising.  It is important to do this on a single cation basis, i.e., $\\mathsf{AlO_{1.5}}$ rather than $\\mathsf{Al_2O_3}$ as the composition will be reported in the raw data, as $X_i$ is counting atoms and there are two Al in every $\\mathsf{Al_2O_3}$.  Expressing this mathematically,\n",
    "\n",
    "\\begin{align}\n",
    "X_i = \\frac{w_i/m_i}{\\sum{w_j/m_j}},\n",
    "\\end{align}\n",
    "\n",
    "where $w_i$ is the weight fraction (or percent) of the oxide, which is how raw compositional data will typically be reported, and $m_i$ is the mean molecular mass of the molecule given in the table below.\n",
    "\n",
    "Finally, $k_d$ is the exchange coefficient of Al between the spinel and olivine and is given by\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{k_d} = \\frac{\\mathsf{Al_2O_3^{olivine}}}{\\mathsf{Al_2O_3^{spinel}}},\n",
    "\\end{align}\n",
    "\n",
    "where the composition here is used as weight percent (i.e., how it will be given in the raw compositional data).\n",
    "\n",
    "|molecule| mean molecular mass |\n",
    "|-----------|------------|\n",
    "|SiO$_2$    | 60.0 |\n",
    "|MgO        | 40.3 |\n",
    "|AlO$_{1.5}$| 51.0 |\n",
    "|TiO$_2$    | 79.9 |\n",
    "|CaO        | 56.0 |\n",
    "|FeO        | 71.8 |\n",
    "|CrO$_{1.5}$| 76.0 |\n",
    "|MnO        | 70.9 |\n",
    "|NiO        | 74.7 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will read in a data file with compositional information from two olivine-spinel pairs, one from Hawaii and one from Iceland, calculate their crystallisation temperatures propagating error on the experimental parameters and compositional data, and decide whether they record different magmatic temperatures between the two settings.  The awkward part of all of this is managing the two data files and making the right parts line up during the calculations -- and it really is awkward, but is a very real challenge in the datasciences. The steps to take are:\n",
    "\n",
    "- Read in the compositional data (file: `crystal_ts.xlsx` using pandas `read_excel`; data are weight percent).\n",
    "- Read in the data file containing the molecular weights (on a single cation basis; file: `molecular_weights.txt` using `read_csv`)\n",
    "- Now, separate the calculation into separate parts.  The simplest thing to do is string all the steps together directly.\n",
    "    - First you will need to take the compositions read in (for olivine and spinel) and randomly perturb them according the error reported in the data files.\n",
    "    - Then calculate the atomic fractions of Cr and Al in the spinel so that you can calculate $Y_\\mathrm{Cr}$.\n",
    "    - Then calculate the $\\mathrm{k_d}$ from the olivine and spinel compositions (using the raw weight percent data of the data file).\n",
    "    - Now perturb the parameters in the themometry equation by their error.\n",
    "    - Finally, calculate the temperature and store it in an array.\n",
    "- Run through that sequence of calculations a large number of times, and end by calculating the mean and standard deviation.\n",
    "\n",
    "### Going further \n",
    "The hard part of this exercise is finding an elegant solution to the data manipulation and calculations that need to be performed.  An inelegant solution will work fine and is probably quicker to code.  But if you want to go further, look into the __[Pandas documentation](https://pandas.pydata.org/docs/user_guide/index.html#user-guide)__ to find out how built in features of the data frames could be used to your advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
